{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gradio as gr\n",
    "import jax\n",
    "import numpy as np\n",
    "import jax.numpy as jnp\n",
    "from flax.training import checkpoints\n",
    "from diffusers import FlaxControlNetModel, FlaxUNet2DConditionModel, FlaxAutoencoderKL, FlaxDDIMScheduler\n",
    "from codi.controlnet_flax import FlaxControlNetModel\n",
    "from codi.pipeline_flax_controlnet import FlaxStableDiffusionControlNetPipeline\n",
    "from transformers import CLIPTokenizer, FlaxCLIPTextModel\n",
    "\n",
    "MODEL_NAME = \"CompVis/stable-diffusion-v1-4\"\n",
    "\n",
    "unet, unet_params = FlaxUNet2DConditionModel.from_pretrained(\n",
    "    MODEL_NAME,\n",
    "    subfolder=\"unet\",\n",
    "    revision=\"flax\",\n",
    "    dtype=jnp.float32,\n",
    ")\n",
    "vae, vae_params = FlaxAutoencoderKL.from_pretrained(\n",
    "    MODEL_NAME,\n",
    "    subfolder=\"vae\",\n",
    "    revision=\"flax\",\n",
    "    dtype=jnp.float32,\n",
    ")\n",
    "text_encoder = FlaxCLIPTextModel.from_pretrained(\n",
    "    MODEL_NAME,\n",
    "    subfolder=\"text_encoder\",\n",
    "    revision=\"flax\",\n",
    "    dtype=jnp.float32,\n",
    ")\n",
    "tokenizer = CLIPTokenizer.from_pretrained(\n",
    "    MODEL_NAME,\n",
    "    subfolder=\"tokenizer\",\n",
    "    revision=\"flax\",\n",
    "    dtype=jnp.float32,\n",
    ")\n",
    "\n",
    "controlnet = FlaxControlNetModel(\n",
    "    in_channels=unet.config.in_channels,\n",
    "    down_block_types=unet.config.down_block_types,\n",
    "    only_cross_attention=unet.config.only_cross_attention,\n",
    "    block_out_channels=unet.config.block_out_channels,\n",
    "    layers_per_block=unet.config.layers_per_block,\n",
    "    attention_head_dim=unet.config.attention_head_dim,\n",
    "    cross_attention_dim=unet.config.cross_attention_dim,\n",
    "    use_linear_projection=unet.config.use_linear_projection,\n",
    "    flip_sin_to_cos=unet.config.flip_sin_to_cos,\n",
    "    freq_shift=unet.config.freq_shift,\n",
    ")\n",
    "scheduler = FlaxDDIMScheduler(\n",
    "    num_train_timesteps=1000,\n",
    "    beta_start=0.00085,\n",
    "    beta_end=0.012,\n",
    "    beta_schedule=\"scaled_linear\",\n",
    "    trained_betas=None,\n",
    "    set_alpha_to_one=True,\n",
    "    steps_offset=0,\n",
    ")\n",
    "scheduler_state = scheduler.create_state()\n",
    "\n",
    "pipeline = FlaxStableDiffusionControlNetPipeline(\n",
    "    vae,\n",
    "    text_encoder,\n",
    "    tokenizer,\n",
    "    unet,\n",
    "    controlnet,\n",
    "    scheduler,\n",
    "    None,\n",
    "    None,\n",
    "    dtype=jnp.float32,\n",
    ")\n",
    "controlnet_params = checkpoints.restore_checkpoint(\"experiments/checkpoint_130001\", target=None)\n",
    "\n",
    "pipeline_params = {\n",
    "    \"vae\": vae_params,\n",
    "    \"unet\": unet_params,\n",
    "    \"text_encoder\": text_encoder.params,\n",
    "    \"scheduler\": scheduler_state,\n",
    "    \"controlnet\": controlnet_params['ema_params']['image_a']['params'],\n",
    "}\n",
    "\n",
    "def infer(prompt, negative_prompt, steps, cfgr):\n",
    "    rng = jax.random.PRNGKey(0)\n",
    "    num_samples = jax.device_count()\n",
    "    rng = jax.random.split(rng, jax.device_count())\n",
    "\n",
    "    prompts = \"Astronaut in a jungle, cold color palette, muted colors, detailed, 8k\"\n",
    "    negative_prompts = \"\"\n",
    "\n",
    "    prompt_ids = pipeline.prepare_text_inputs([prompts] * num_samples)\n",
    "    negative_prompt_ids = pipeline.prepare_text_inputs([negative_prompts] * num_samples)\n",
    "\n",
    "    output = pipeline(\n",
    "        prompt_ids=prompt_ids,\n",
    "        image=None,\n",
    "        params=pipeline_params,\n",
    "        prng_seed=rng,\n",
    "        num_inference_steps=4,\n",
    "        guidance_scale=6.5,\n",
    "        neg_prompt_ids=negative_prompt_ids,\n",
    "        jit=False,\n",
    "    ).images\n",
    "\n",
    "    output_images = pipeline.numpy_to_pil(np.asarray(output.reshape((num_samples,) + output.shape[-3:])))\n",
    "    return output_images\n",
    "\n",
    "with gr.Blocks(theme='gradio/soft') as demo:\n",
    "    gr.Markdown(\"## Conditional Distillation (CoDi) with Different Controls\")\n",
    "    gr.Markdown(\"[\\[Paper\\]](https://arxiv.org/abs/2310.01407) [\\[Project Page\\]](https://fast-codi.github.io)\")\n",
    "\n",
    "    with gr.Tab(\"CoDi on Text-to-Image\"):\n",
    "        \n",
    "        with gr.Row():\n",
    "            with gr.Column():\n",
    "                prompt_input = gr.Textbox(label=\"Prompt\")\n",
    "                negative_prompt = gr.Textbox(label=\"Negative Prompt\", value=\"monochrome, lowres, bad anatomy, worst quality, low quality\")\n",
    "            output = gr.Gallery(label=\"Output Images\")\n",
    "\n",
    "        with gr.Row():\n",
    "            num_inference_steps = gr.Slider(2, 8, value=4, step=1, label=\"Steps\")\n",
    "            guidance_scale = gr.Slider(2.0, 14.0, value=7.5, step=0.5, label='Guidance Scale')\n",
    "        submit_btn = gr.Button(value = \"Submit\")\n",
    "        inputs = [\n",
    "            prompt_input,\n",
    "            negative_prompt,\n",
    "            num_inference_steps,\n",
    "            guidance_scale\n",
    "        ]\n",
    "        submit_btn.click(fn=infer, inputs=inputs, outputs=[output])\n",
    "\n",
    "        with gr.Row():\n",
    "            gr.Examples(\n",
    "                examples=[\"oranges\", \"Astronaut in a jungle, cold color palette, muted colors, detailed, 8k\"],\n",
    "                inputs=prompt_input,\n",
    "                fn=infer\n",
    "            )\n",
    "\n",
    "demo.launch()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
